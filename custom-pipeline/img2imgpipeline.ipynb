{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from typing import Union\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers.models import AutoencoderKL, UNet2DConditionModel\n",
    "from diffusers.schedulers import DDPMScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"../1_media/input_images/cat.jpg\").convert(\"RGB\")\n",
    "\n",
    "prompt = \"a beautiful cat with a human face\"\n",
    "negative_prompt = None\n",
    "seed = -1\n",
    "guidance_scale = 5\n",
    "num_inference_steps = 4\n",
    "strength = 0.6\n",
    "num_images_per_prompt = 1\n",
    "batch_size = 1\n",
    "device = \"cuda\"\n",
    "dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "vae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"vae\").to(device=device, dtype=dtype)\n",
    "unet = UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"unet\").to(device=device, dtype=dtype)\n",
    "scheduler = DDPMScheduler.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* padding :\n",
    "\n",
    "    * \"max_length\" : Belirtilen maksimum uzunluğa veya model için kabul edilebilir maksimum giriş uzunluğuna kadar padding yapılır.\n",
    "    * \"longest\" veya True : Prompt batch halinde verildiyse, batch içindeki promptlardan en uzun olanına göre padding yapılır. Eğer bir prompt varsa padding yapılmaz.\n",
    "    * \"do_not_pad\" veya False : Padding yapılmaz.\n",
    "\n",
    "\n",
    "* max_length : Padding veya truncation için maksimum uzunluk. Eğer padding veya truncation yapılacaksa bu değer kullanılır. Ayarlanmadan bırakılırsa veya None olarak ayarlanırsa, model için kabul edilebilir maksimum giriş uzunluğu kullanılır.\n",
    "\n",
    "* truncation :\n",
    "\n",
    "    * True veya \"longest_first\" : max_length değişkeniyle verilen maksimum uzunluğa veya bu değişken sağlanmamışsa model için kabul edilebilir maksimum uzunluğa kadar kesme yapılır. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_prompt(tokenizer : CLIPTokenizer, \n",
    "              text_encoder : CLIPTextModel, \n",
    "              prompt : Union[str,list], \n",
    "              negative_prompt : Union[str,list], \n",
    "              num_images_per_prompt : int, \n",
    "              cfg: bool):\n",
    "\n",
    "    if prompt is not None and isinstance(prompt, str):\n",
    "        batch_size = 1\n",
    "    elif prompt is not None and isinstance(prompt, list):\n",
    "        batch_size = len(prompt)\n",
    "    else:\n",
    "        batch_size = prompt_embeds.shape[0]\n",
    "   \n",
    "    # Prompt tokenize\n",
    "    tokens = tokenizer(prompt,\n",
    "                       padding=\"max_length\",\n",
    "                       max_length=tokenizer.model_max_length,\n",
    "                       truncation=True,\n",
    "                       return_tensors=\"pt\")\n",
    "    \n",
    "    #print(\"Tokens:\", tokens, \"attention_mask hakkinda daha fazla bilgi icin: https://huggingface.co/transformers/glossary.html#attention-mask\")\n",
    "        \n",
    "    tokens_ids = tokens.input_ids\n",
    "    print(\"Tokens IDs:\", tokens_ids)\n",
    "\n",
    "\n",
    "    # Untrancated hesabı, gerek yok, diffusers kesilen prompt parçasını warning basmak için hesaplıyor.\n",
    "    \"\"\" untruncated_ids = tokenizer(prompt, padding=\"do_not_pad\", return_tensors=\"pt\").input_ids\n",
    "    print(\"Untrancated IDs:\", untruncated_ids)\n",
    "\n",
    "    if untruncated_ids.shape[-1] >= tokens_ids.shape[-1] and not torch.equal(tokens_ids, untruncated_ids):\n",
    "        removed_text = tokenizer.batch_decode(untruncated_ids[:, tokenizer.model_max_length - 1 : -1])\n",
    "        print(\"Removed Text:\", removed_text) \"\"\"\n",
    "\n",
    "    # Model text encoder config dosyasında use_attention_mask varsa ve True ise attention maskı kullan.\n",
    "    if hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask:\n",
    "        attention_mask = tokens.attention_mask\n",
    "        \n",
    "    else:\n",
    "        attention_mask = None\n",
    "    print(\"Attention Mask:\", attention_mask)\n",
    "\n",
    "\n",
    "    # Prompt embeds\n",
    "    prompt_embeds = text_encoder(tokens_ids, attention_mask=attention_mask) # text encoder encode edilmiş promptu döner\n",
    "    prompt_embeds = prompt_embeds[0]\n",
    "   \n",
    "    print(\"Prompt Embeds:\", prompt_embeds.shape)\n",
    "\n",
    "    # duplicate prompt embeds for each image\n",
    "    bs_embed, seq_len, _ = prompt_embeds.shape\n",
    "    prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "    print(\"Prompt Embeds Repeated Shape:\", prompt_embeds.shape)\n",
    "\n",
    "    prompt_embeds = prompt_embeds.view(bs_embed * num_images_per_prompt, seq_len, -1)\n",
    "    print(\"Prompt Embeds View Shape:\", prompt_embeds.shape)\n",
    "\n",
    "    # negative prompt\n",
    "    if negative_prompt is None:\n",
    "        uncond_tokens = [\"\"] * batch_size\n",
    "\n",
    "    elif isinstance(negative_prompt, str):\n",
    "        uncond_tokens = [negative_prompt]\n",
    "    else:\n",
    "        uncond_tokens = negative_prompt\n",
    "    \n",
    "    # negative prompt tokenize\n",
    "    max_length = prompt_embeds.shape[1]\n",
    "    uncond_tokens = tokenizer(uncond_tokens,\n",
    "                             padding=\"max_length\",\n",
    "                             max_length=max_length,\n",
    "                             truncation=True,\n",
    "                             return_tensors=\"pt\")\n",
    "\n",
    "    if hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask:\n",
    "        attention_mask = uncond_tokens.attention_mask\n",
    "    else:\n",
    "        attention_mask = None\n",
    "\n",
    "    uncond_tokens_ids = uncond_tokens.input_ids\n",
    "    negative_prompt_embeds = text_encoder(uncond_tokens_ids, attention_mask=attention_mask)\n",
    "    negative_prompt_embeds = negative_prompt_embeds[0]\n",
    "\n",
    "    if cfg:\n",
    "        seq_len = negative_prompt_embeds.shape[1]\n",
    "\n",
    "        negative_prompt_embeds = negative_prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "        negative_prompt_embeds = negative_prompt_embeds.view(bs_embed * num_images_per_prompt, seq_len, -1)\n",
    "            \n",
    "        prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])\n",
    "\n",
    "    return prompt_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_embeds = encode_prompt(tokenizer, text_encoder, prompt, negative_prompt, num_images_per_prompt = 1, cfg = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocess(image: Image, height: int, width: int, device: torch.device, dtype: torch.dtype):\n",
    "    \n",
    "    image = image.resize((width, height), Image.LANCZOS)\n",
    "\n",
    "    # pil to np\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "\n",
    "    # np to torch pt\n",
    "    if image.ndim == 3:\n",
    "        image = image[..., None]\n",
    "\n",
    "    image = torch.from_numpy(image.transpose(3,2,0,1))\n",
    "    \n",
    "    # expected range [0,1], normalize to [-1,1]\n",
    "    if image.min() < 0:\n",
    "        print(\n",
    "            \"Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] \"\n",
    "            f\"when passing as pytorch tensor or numpy Array. You passed `image` with value range [{image.min()},{image.max()}]\",\n",
    "            FutureWarning,\n",
    "        )\n",
    "\n",
    "    image = 2.0 * image - 1.0\n",
    "    image = image.to(device=device, dtype=dtype)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_postprocess(image: torch.FloatTensor):\n",
    "        \n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        \n",
    "        # torch pt to np\n",
    "        image = image.cpu().permute(0, 2, 3, 1).float().detach().numpy()\n",
    "\n",
    "        if image.ndim == 3:\n",
    "            image = image[..., None]\n",
    "\n",
    "        image = torch.from_numpy(image.transpose(0, 3, 1, 2))\n",
    "        image = image.squeeze(0)\n",
    "        pil_image = transforms.ToPILImage()(image)  \n",
    "\n",
    "        return pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timesteps(scheduler, num_inference_steps, strength):\n",
    "        \n",
    "        init_timestep = min(int(num_inference_steps * strength), num_inference_steps)\n",
    "        t_start = max(num_inference_steps - init_timestep, 0)\n",
    "        timesteps = scheduler.timesteps[int(t_start) * scheduler.order : ]\n",
    "        \n",
    "        return timesteps, num_inference_steps - t_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_latents(image, timestep, batch_size, num_images_per_prompt, dtype, device, vae, scheduler, generator=None):\n",
    "        print(\"Prepare Latents\")\n",
    "        batch_size = batch_size * num_images_per_prompt\n",
    "\n",
    "        if image.shape[1] == 4:\n",
    "            init_latents = image\n",
    "\n",
    "        else:\n",
    "            init_latents = vae.encode(image).latent_dist.sample(generator)\n",
    "            init_latents = vae.config.scaling_factor * init_latents\n",
    "            init_latents = torch.cat([init_latents], dim=0)\n",
    "\n",
    "        shape = init_latents.shape\n",
    "        noise = torch.randn(shape, generator=generator, device=device, dtype=dtype, layout=torch.strided)\n",
    "        # get latents\n",
    "        init_latents = scheduler.add_noise(init_latents, noise, timestep)\n",
    "        latents = init_latents\n",
    "\n",
    "        return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_pipeline(vae,\n",
    "                 unet,\n",
    "                 scheduler,\n",
    "                 tokenizer,\n",
    "                 text_encoder,\n",
    "                 image, \n",
    "                 prompt,\n",
    "                 negative_prompt, \n",
    "                 seed, \n",
    "                 guidance_scale: float = 7.5,\n",
    "                 num_inference_steps: int = 25, \n",
    "                 strength: float = 0.5, \n",
    "                 num_images_per_prompt: int = 1, \n",
    "                 batch_size: int = 1, \n",
    "                 device: str = \"cuda\",\n",
    "                 dtype: torch.dtype = torch.float16):\n",
    "\n",
    "    # seedi ayarla\n",
    "    if seed == -1:\n",
    "        seed = np.random.randint(0, 1000000)\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "\n",
    "    # guidance scale ayarla\n",
    "    cfg = guidance_scale > 1\n",
    "\n",
    "    # encode prompt\n",
    "    prompt_embeds = encode_prompt(tokenizer, text_encoder, prompt, negative_prompt, num_images_per_prompt, cfg)\n",
    "    prompt_embeds = prompt_embeds.to(device=device, dtype=dtype)\n",
    "    height = image.size[1]\n",
    "    width = image.size[0]\n",
    "    print(\"Image Size:\", height, width)\n",
    "    # image preprocess\n",
    "    image = image_preprocess(image, height=height, width=width, device=device, dtype=dtype)\n",
    "\n",
    "    # create scheduler timesteps\n",
    "    scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "    timesteps, num_inference_steps = get_timesteps(scheduler,num_inference_steps, strength)\n",
    "    latent_timestep = timesteps[:1].repeat(batch_size * num_images_per_prompt)\n",
    "\n",
    "    # latents kısmı\n",
    "    latents = prepare_latents(image=image,\n",
    "                              timestep=latent_timestep,\n",
    "                              batch_size=batch_size,\n",
    "                              num_images_per_prompt=num_images_per_prompt,\n",
    "                              dtype=prompt_embeds.dtype,\n",
    "                              device=device,\n",
    "                              vae=vae,\n",
    "                              scheduler=scheduler,\n",
    "                              generator=generator)\n",
    "    \n",
    "    print(\"Latents Shape:\", latents.shape)\n",
    "\n",
    "    # unet loop kısmı\n",
    "    for i, t in enumerate(timesteps):\n",
    "       \n",
    "        latent_model_input = torch.cat([latents] * 2) if cfg else latents\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "       \n",
    "        # unet forward\n",
    "        noise_pred = unet(\n",
    "            latent_model_input,\n",
    "            t,\n",
    "            encoder_hidden_states=prompt_embeds,\n",
    "            return_dict=False\n",
    "        )[0]\n",
    "\n",
    "        # perform guidance\n",
    "        if cfg:\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "        # compute the previous noisy sample x_t -> x_t-1\n",
    "        latents = scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n",
    "\n",
    "        print(f\"{i}/{num_inference_steps} latents: {latents.shape}\")\n",
    "\n",
    "    image = vae.decode(latents / vae.config.scaling_factor, return_dict=False)[0]\n",
    "    image = image_postprocess(image)\n",
    "    \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_image = custom_pipeline(vae=vae,\n",
    "                               unet=unet,\n",
    "                               scheduler=scheduler,\n",
    "                               tokenizer=tokenizer,\n",
    "                               text_encoder=text_encoder,\n",
    "                               image=image,\n",
    "                               prompt=prompt,\n",
    "                               negative_prompt=negative_prompt,\n",
    "                               seed=seed,\n",
    "                               guidance_scale=guidance_scale,\n",
    "                               num_inference_steps=num_inference_steps,\n",
    "                               strength=strength,\n",
    "                               num_images_per_prompt=num_images_per_prompt,\n",
    "                               batch_size=batch_size,\n",
    "                               device=device,\n",
    "                               dtype=dtype)\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize = (20, 20), facecolor = 'white')\n",
    "fig.add_subplot(1, 2, 1)\n",
    "  \n",
    "# showing image\n",
    "plt.imshow(image)\n",
    "plt.axis('on')\n",
    "plt.title(\"Original\")\n",
    "  \n",
    "# Adds a subplot at the 2nd position\n",
    "fig.add_subplot(1, 2, 2)\n",
    "  \n",
    "# showing image\n",
    "plt.imshow(generated_image)\n",
    "plt.axis('on')\n",
    "plt.title(\"Generated\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63ee1a40d545eb9ed0dc670dd2dc022cb15a164f5c0cf5386c7575814bbda051"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
