{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from diffusers import StableDiffusionImg2ImgPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = Image.open(\"../1_media/input_images/cat.jpg\").convert(\"RGB\")\n",
    "input_prompt = \"a cat in the style of a painting\"\n",
    "\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
    "pipe = pipe.to(\"cuda\", dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "generator = torch.manual_seed(seed)\n",
    "\n",
    "input_images = []\n",
    "original_captions = []\n",
    "modified_captions = []\n",
    "edited_images = []\n",
    "\n",
    "input_image = input_image\n",
    "edit_instruction = input_prompt\n",
    "edited_image = pipe(\n",
    "        prompt=edit_instruction,\n",
    "        image=input_image,\n",
    "        output_type=\"pil\",\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "edited_image.save(f\"output.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_images.append(np.array(input_image))\n",
    "original_captions.append(\"a cat\")\n",
    "modified_captions.append(\"a dog\")\n",
    "edited_images.append(edited_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    CLIPTokenizer,\n",
    "    CLIPTextModelWithProjection,\n",
    "    CLIPVisionModelWithProjection,\n",
    "    CLIPImageProcessor,\n",
    ")\n",
    "\n",
    "clip_id = \"openai/clip-vit-large-patch14\"\n",
    "tokenizer = CLIPTokenizer.from_pretrained(clip_id)\n",
    "text_encoder = CLIPTextModelWithProjection.from_pretrained(clip_id).to(\"cuda\")\n",
    "image_processor = CLIPImageProcessor.from_pretrained(clip_id)\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(clip_id).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DirectionalSimilarity(nn.Module):\n",
    "    def __init__(self, tokenizer, text_encoder, image_processor, image_encoder):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_encoder = text_encoder\n",
    "        self.image_processor = image_processor\n",
    "        self.image_encoder = image_encoder\n",
    "\n",
    "    def preprocess_image(self, image):\n",
    "        image = self.image_processor(image, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "        return {\"pixel_values\": image.to(\"cuda\")}\n",
    "\n",
    "    def tokenize_text(self, text):\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\"input_ids\": inputs.input_ids.to(\"cuda\")}\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        preprocessed_image = self.preprocess_image(image)\n",
    "        image_features = self.image_encoder(**preprocessed_image).image_embeds\n",
    "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "        return image_features\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        tokenized_text = self.tokenize_text(text)\n",
    "        text_features = self.text_encoder(**tokenized_text).text_embeds\n",
    "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "        return text_features\n",
    "\n",
    "    def compute_directional_similarity(self, img_feat_one, img_feat_two, text_feat_one, text_feat_two):\n",
    "        sim_direction = F.cosine_similarity(img_feat_two - img_feat_one, text_feat_two - text_feat_one)\n",
    "        return sim_direction\n",
    "\n",
    "    def forward(self, image_one, image_two, caption_one, caption_two):\n",
    "        img_feat_one = self.encode_image(image_one)\n",
    "        img_feat_two = self.encode_image(image_two)\n",
    "        text_feat_one = self.encode_text(caption_one)\n",
    "        text_feat_two = self.encode_text(caption_two)\n",
    "        directional_similarity = self.compute_directional_similarity(\n",
    "            img_feat_one, img_feat_two, text_feat_one, text_feat_two\n",
    "        )\n",
    "        return directional_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_similarity = DirectionalSimilarity(tokenizer, text_encoder, image_processor, image_encoder)\n",
    "scores = []\n",
    "\n",
    "for i in range(len(input_images)):\n",
    "    original_image = input_images[i]\n",
    "    original_caption = original_captions[i]\n",
    "    edited_image = edited_images[i]\n",
    "    modified_caption = modified_captions[i]\n",
    "\n",
    "    similarity_score = dir_similarity(original_image, edited_image, original_caption, modified_caption)\n",
    "    scores.append(float(similarity_score.detach().cpu()))\n",
    "\n",
    "print(f\"CLIP directional similarity: {np.mean(scores)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
